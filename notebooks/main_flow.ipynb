{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bed027",
   "metadata": {},
   "source": [
    "# Project\n",
    " - 2 CLUSTERING AND 1 DIMENSION REDUCTION METHOD NOT LEARNT IN THE COURSE \n",
    "\n",
    "## Static Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f5a82e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T16:38:50.630707Z",
     "start_time": "2023-02-23T16:38:49.949222Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Tuple, Dict, Any, List\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# from src.algorithms import clustering_algorithms, dim_reduction_algorithms, anomaly_detection_algorithms\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import ttest_rel\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d447cbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T16:38:51.209599Z",
     "start_time": "2023-02-23T16:38:50.785706Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, Birch\n",
    "from sklearn.manifold import TSNE, SpectralEmbedding, Isomap, MDS\n",
    "from fcmeans import FCM\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "import hdbscan\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "\n",
    "def k_mean(k, data: np.ndarray):\n",
    "    model = KMeans(k, n_init=\"auto\", max_iter=1000)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def fuzzy_c_means(k, data: np.ndarray):\n",
    "    model = FCM(n_clusters=k, n_jobs=8)\n",
    "    model.fit(data)\n",
    "    return model.predict(data)\n",
    "\n",
    "\n",
    "def gaussian_mixture(k: int, data: np.ndarray):\n",
    "    model = GaussianMixture(k, max_iter=3000)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def hierarchical_clustering(k: int, data: np.ndarray):\n",
    "    model = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def birch(k: int, data: np.ndarray):\n",
    "    model = Birch(n_clusters=k)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def spectral_clustering(k: int, data: np.ndarray):\n",
    "    model = SpectralClustering(n_clusters=k, assign_labels='discretize')\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def hierarchical_dbscan(k: int, data: np.ndarray):\n",
    "    model = hdbscan.HDBSCAN()\n",
    "    model.fit(data)\n",
    "    return model.labels_\n",
    "\n",
    "\n",
    "def get_best_dbscan_params(clean_X: np.ndarray, min_samples: int):\n",
    "    range_eps = np.linspace(0.1, 10, 20)\n",
    "    scores = []\n",
    "    for eps in range_eps:\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        good_labels = model.fit_predict(clean_X)\n",
    "        noisy_data_count = len(good_labels[good_labels == -1])\n",
    "        if noisy_data_count > good_labels.shape[0] * 0.5 or len(np.unique(good_labels)) < 3:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        print(f\"noisy data count with eps={eps}: {noisy_data_count}\")\n",
    "        try:\n",
    "            score = silhouette_score(clean_X, good_labels)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    plt.plot(range_eps, scores)\n",
    "    plt.show()\n",
    "    return range_eps[np.argmax(scores)]\n",
    "\n",
    "\n",
    "def visualize_hierarchical_clustering(data: np.ndarray):\n",
    "    sch.dendrogram(sch.linkage(data, method='ward'))\n",
    "    plt.title('Dendrogram')\n",
    "    plt.xlabel('Customers')\n",
    "    plt.ylabel('Euclidean distances')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "clustering_algorithms = {\n",
    "    \"k_mean\": k_mean,\n",
    "    \"fuzzy_c_means\": fuzzy_c_means,\n",
    "    \"gaussian_mixture\": gaussian_mixture,\n",
    "    \"hierarchical_clustering\": hierarchical_clustering,\n",
    "    \"birch\": birch,\n",
    "    \"spectral_clustering\": spectral_clustering,\n",
    "    \"hierarchical_dbscan\": hierarchical_dbscan,\n",
    "}\n",
    "\n",
    "dim_reduction_algorithms = {\n",
    "#     \"TSNE\": lambda k: TSNE(n_components=k, method=\"exact\"),\n",
    "#     \"Isomap\": lambda k: Isomap(n_components=k),\n",
    "#     \"MDS\": lambda k: MDS(n_components=k),\n",
    "    \"SpectralEmbedding\": lambda k: SpectralEmbedding(n_components=k),\n",
    "    \"PCA\": lambda k: PCA(n_components=k),\n",
    "    \"FastICA\": lambda k: FastICA(n_components=k),\n",
    "    \"without_reduction\": None\n",
    "}\n",
    "\n",
    "anomaly_detection_algorithms = {\n",
    "    \"OneClassSVM\": OneClassSVM(),\n",
    "    \"IsolationForest\": IsolationForest(n_estimators=500),\n",
    "    \"DBSCAN\": DBSCAN(eps=0.4, min_samples=10)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cce239fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:32:12.509745Z",
     "start_time": "2023-02-23T17:32:06.424074Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"output/\"\n",
    "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Config\n",
    "random_state = 0\n",
    "data_path = \"../data/fma_metadata\"\n",
    "features = pd.read_csv(os.path.join(data_path, \"features.csv\"), index_col=0, header=[0, 1, 2])\n",
    "tracks = pd.read_csv(os.path.join(data_path, \"tracks.csv\"), index_col=0, header=[0, 1])\n",
    "dimentions_options = [10, None]\n",
    "num_clusters_options = list(range(2, 20, 10))\n",
    "num_of_cvs = 5\n",
    "cv_size = 100\n",
    "p_value_thr = 0.05\n",
    "external_vars = [('track', 'genre_top'), ('track', 'license'), ('album', 'type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0325d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:32:23.099647Z",
     "start_time": "2023-02-23T17:32:22.984854Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in external_vars:\n",
    "    tracks[column] = tracks[column].astype('category')\n",
    "features.columns = ['_'.join(col) for col in features.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eab3cb2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:32:23.699693Z",
     "start_time": "2023-02-23T17:32:23.688619Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('track', 'title'),\n",
       " ('artist', 'active_year_begin'),\n",
       " ('track', 'information'),\n",
       " ('album', 'producer'),\n",
       " ('artist', 'associated_labels'),\n",
       " ('set', 'subset'),\n",
       " ('album', 'title'),\n",
       " ('set', 'split'),\n",
       " ('artist', 'members'),\n",
       " ('artist', 'location'),\n",
       " ('album', 'information'),\n",
       " ('track', 'date_created'),\n",
       " ('track', 'genres_all'),\n",
       " ('artist', 'active_year_end'),\n",
       " ('track', 'date_recorded'),\n",
       " ('track', 'language_code'),\n",
       " ('track', 'tags'),\n",
       " ('artist', 'wikipedia_page'),\n",
       " ('track', 'lyricist'),\n",
       " ('album', 'date_released'),\n",
       " ('artist', 'name'),\n",
       " ('album', 'date_created'),\n",
       " ('artist', 'website'),\n",
       " ('artist', 'related_projects'),\n",
       " ('track', 'genres'),\n",
       " ('track', 'genre_top'),\n",
       " ('artist', 'date_created'),\n",
       " ('album', 'tags'),\n",
       " ('track', 'publisher'),\n",
       " ('track', 'license'),\n",
       " ('track', 'composer'),\n",
       " ('artist', 'tags'),\n",
       " ('album', 'engineer'),\n",
       " ('album', 'type'),\n",
       " ('artist', 'bio')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(tracks.columns) - set(tracks._get_numeric_data().columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01090b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:32:24.438340Z",
     "start_time": "2023-02-23T17:32:24.316219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('track', 'genre_top') 16\n",
      "('track', 'license') 113\n",
      "('album', 'type') 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chroma_cens_kurtosis_01        0\n",
       "chroma_cens_kurtosis_02        0\n",
       "chroma_cens_kurtosis_03        0\n",
       "chroma_cens_kurtosis_04        0\n",
       "chroma_cens_kurtosis_05        0\n",
       "                           ...  \n",
       "zcr_skew_01                    0\n",
       "zcr_std_01                     0\n",
       "(track, genre_top)         56976\n",
       "(track, license)              87\n",
       "(album, type)               6508\n",
       "Length: 521, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([features, tracks[external_vars]],axis=1)\n",
    "for column in external_vars:\n",
    "    print(column, df[column].nunique())\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e6491a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:33:57.019073Z",
     "start_time": "2023-02-23T17:33:56.918580Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "X, y = df.drop(external_vars, axis=1), df[external_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a742158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:35:18.800223Z",
     "start_time": "2023-02-23T17:35:18.766554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47492, 518) (47492, 3)\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y. shape)\n",
    "print((X.isna().sum() != 0).any())\n",
    "print((y.isna().sum() != 0).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc18801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T16:39:14.496754Z",
     "start_time": "2023-02-23T16:39:14.490605Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_cvs(X: np.ndarray, y: pd.DataFrame):\n",
    "    X_cvs = []\n",
    "    y_cvs = []\n",
    "    for i in range(num_of_cvs):\n",
    "        rows = np.random.randint(cv_size, size=X.shape[0]).astype('bool')\n",
    "        X_cvs.append(X[rows])\n",
    "        y_cvs.append(y[rows])\n",
    "    return X_cvs, y_cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e67ec0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T16:53:22.748448Z",
     "start_time": "2023-02-23T16:53:22.726235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('track', 'genre_top') 16\n",
      "('track', 'license') 113\n",
      "('album', 'type') 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100, 518), (100, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_data()\n",
    "rows = np.random.randint(X.shape[0], size=cv_size)\n",
    "X, y = X.iloc[rows].values, y.iloc[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f1a2c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-23T16:37:59.399Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                                                           | 0/7 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "def reduction_algo_wrapper(reduction_algo_name: str, dim_num: int, cv_data: np.ndarray, cv_id: int) -> np.ndarray:\n",
    "    reduction_algo = dim_reduction_algorithms[reduction_algo_name]\n",
    "    if reduction_algo is None:\n",
    "        return cv_data\n",
    "    cache_file = f\"{reduction_algo_name}-{dim_num}-{cv_id}.pkl\"\n",
    "    if Path(cache_file).is_file():\n",
    "        print(f\"using cache file {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    cv_data = reduction_algo(dim_num).fit_transform(cv_data)\n",
    "    with open(cache_file, \"wb\") as file:\n",
    "        pickle.dump(cv_data, file)\n",
    "    return cv_data\n",
    "\n",
    "\n",
    "def main_flow(X_cvs: List[np.ndarray]) -> Dict[str, Dict[str, Any]]:\n",
    "    best_config_by_clustering = dict()\n",
    "    for clustering_algo_name, clustering_algo in clustering_algorithms.items():\n",
    "        dim_reduction_scores = dict()\n",
    "        dim_reduction_meta = dict()\n",
    "        for reduction_algo_name in tqdm(dim_reduction_algorithms.keys()):\n",
    "            max_score = float(\"-inf\")\n",
    "            for dim_num in dimentions_options:\n",
    "                for k_clusters in num_clusters_options:\n",
    "                    scores = []\n",
    "                    for cv_id, cv_data in enumerate(X_cvs):\n",
    "                        cv_data = reduction_algo_wrapper(reduction_algo_name,dim_num,cv_data, cv_id)\n",
    "                        print(f\"doing {clustering_algo_name} for {cv_data.shape} by {reduction_algo_name}\")\n",
    "                        labels = clustering_algo(k_clusters, cv_data)\n",
    "                        scores.append(silhouette_score(cv_data, labels))\n",
    "                    curr_score = np.mean(scores)\n",
    "                    if curr_score > max_score:\n",
    "                        max_score = curr_score\n",
    "                        dim_reduction_scores[reduction_algo_name] = scores\n",
    "                        dim_reduction_meta[reduction_algo_name] = {\n",
    "                            \"max_score\": max_score,\n",
    "                            \"max_dim_num\": dim_num,\n",
    "                            \"max_cluster_num\": k_clusters,\n",
    "                            \"scores\": scores,\n",
    "                            \"reduction_algo_name\": reduction_algo_name\n",
    "                        }\n",
    "\n",
    "        _, p_value = f_oneway(list(dim_reduction_scores.values()))\n",
    "        best_config_by_clustering[clustering_algo_name] = random.choice(list(dim_reduction_meta.values()))\n",
    "        if p_value < p_value_thr:\n",
    "            sorted_reduction_algos = sorted(\n",
    "                dim_reduction_meta,\n",
    "                key=lambda key: dim_reduction_meta[key][\"max_score\"],\n",
    "                reverse=True\n",
    "            )\n",
    "            reduction_algo_name_1, reduction_algo_name_2 = sorted_reduction_algos[:2]\n",
    "            _, t_test_p_value = ttest_rel(\n",
    "                dim_reduction_scores[reduction_algo_name_1],\n",
    "                dim_reduction_scores[reduction_algo_name_2]\n",
    "            )\n",
    "            best_algo_name: str = sorted_reduction_algos[0]\n",
    "            best_config_by_clustering[clustering_algo_name] = dim_reduction_meta[best_algo_name]\n",
    "            if t_test_p_value >= p_value_thr:\n",
    "                print(f\"followed by t-test: algorithms {reduction_algo_name_1}, {reduction_algo_name_2} are the same\")\n",
    "        else:\n",
    "            print(f\"followed by annova: algorithms {dim_reduction_scores.keys()} are the same\")\n",
    "        print(f\"picking for {clustering_algo_name}: {best_config_by_clustering[clustering_algo_name]}\")\n",
    "        print(best_config_by_clustering)\n",
    "        with open(\"output/best_config_by_clustering\", \"w\") as file:\n",
    "            json.dump(best_config_by_clustering, file)\n",
    "    return best_config_by_clustering\n",
    "\n",
    "best_config_by_clustering = main_flow(X_cvs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4004d4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T17:38:31.316545Z",
     "start_time": "2023-02-22T17:38:31.311365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106574, 518)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedab5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = [Kmean(), ...]\n",
    "dim_reduction_algo = [PCA(), ICA(),...]\n",
    "dimentions_options = [10, 50, 200, None]\n",
    "num_clusters_options = [2, 4, 6, 8, 10, 15, 20]\n",
    "best_config_by_clustering = dict()\n",
    "for clutering_algo in scores:\n",
    "    for dim_reduction in dim_reduction_algo:\n",
    "        max_score = 0\n",
    "        dim_reduction_distrabutions = defualtdict(list)\n",
    "        for dim_num in dimentions_options:\n",
    "            for cluster_num in num_clusters_options:\n",
    "                scores = []\n",
    "                for cv_data in cvs:\n",
    "                    new_data = dim_reduction(cv_data, dim_num)\n",
    "                    labdels = clutering(new_data)\n",
    "                    scores.append(sillout(labels))\n",
    "                curr_score = np.mean(scores)\n",
    "                if curr_score > max_score:\n",
    "                    max_score = curr_score\n",
    "                    dim_reduction_distrabutions[dim_reduction, \"scores\"] = scores\n",
    "                    dim_reduction_distrabutions[dim_reduction, \"mean_score\"] = max_score\n",
    "                    dim_reduction_distrabutions[dim_reduction, \"max_dim_num\"] = dim_num\n",
    "                    dim_reduction_distrabutions[dim_reduction, \"max_cluster_num\"] = cluster_num\n",
    "                    # not really metters\n",
    "\n",
    "    p_value = anova(dim_reduction_distrabutions.values())\n",
    "    if p_value < 0.05:\n",
    "        dim_reduction1, dim_reduction2 = arg_n_top(n=2, dim_reduction_distrabutions[:, \"mean_score\"])\n",
    "        t_p_value = t_test(dim_reduction_distrabutions[dim_reduction1, \"scores\"], dim_reduction_distrabutions[dim_reduction2, \"scores\"])\n",
    "        if t_p_value < 0.05:\n",
    "            best_dim_reduction = argmax(dim_reduction_distrabutions[:, \"mean_score\"])\n",
    "            best_config_by_clustering[clutering_algo] = best_dim_reduction # reduction_algo,  max_dim_num, max_cluster_num\n",
    "            # algo reduction , dimention, n_cluster\n",
    "        else:\n",
    "            print(\"same same but different\")\n",
    "    else:\n",
    "        print(\"same same but different\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden variables with the best clustering and dim_reduction\n",
    "for external_var in external_vars:\n",
    "    all_mi = dict()\n",
    "    for clustering_algo, best_config in best_config_by_clustering.items():\n",
    "        for cv in cvs:\n",
    "            labels = clustering_algo(best_config, cv)\n",
    "            all_mi[external_var].append(MI(labels, external_var))\n",
    "    anova(all_mi.values())\n",
    "    # t-test with the top 2\n",
    "    # yield best_cluster_algo_per_external_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704359da",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mi = dict()\n",
    "for clustering_algo, best_config in best_config_by_clustering.items():\n",
    "    for external_var in external_vars:\n",
    "        for cv in cvs:\n",
    "            # change in best_config max_cluster_num to len(np.unique(external_var))\n",
    "            labels = clustering_algo(best_config, cv)\n",
    "            all_mi[clustering_algo, external_var].append(MI(labels, external_var))\n",
    "    anova(all_mi[clustering_algo, :].values())\n",
    "    # t-test with the top 2\n",
    "    # yield best_cluster_algo_per_external_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885f710",
   "metadata": {},
   "source": [
    "### Cluster the remaining columns, and explain what are is the best clustering method.\n",
    "- silhouette_score\n",
    "- elbow method\n",
    "- k-fold cross validation on silhouette_score or Matual information with - kruskal wallis or anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_by_silhouette(models, X)\n",
    "    for model in models:\n",
    "        labels = model.fit_predict(X)\n",
    "        scores.append(metrics.silhouette_score(X, labels))\n",
    "    plt.plot(models, scores)\n",
    "    plt.title(f\"silhouette_score diffrent models - {model_name}\")\n",
    "    plt.xlabel(\"models\")\n",
    "    plt.ylabel(\"Silhouette Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f908f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a0a632",
   "metadata": {},
   "source": [
    "### Fuzzy C-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a6931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = FCM(n_clusters=2)\n",
    "my_model.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af32d6",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a4599",
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans(k, n_init=\"auto\", random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725b031",
   "metadata": {},
   "source": [
    "#### Gaussian-Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52861b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussianMixture(k, max_iter=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28878168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T13:25:56.391773Z",
     "start_time": "2023-02-03T13:25:56.384378Z"
    }
   },
   "source": [
    "#### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965558d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as sch # importing scipy.cluster.hierarchy for dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward')) # finding the optimal number of clusters using dendrogram\n",
    "plt.title('Dendrogram') # title of the dendrogram\n",
    "plt.xlabel('Customers') # label of the x-axis\n",
    "plt.ylabel('Euclidean distances') # label of the y-axis\n",
    "plt.show() # show the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Agg_hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\n",
    "y_hc = Agg_hc.fit_predict(newData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e464f31",
   "metadata": {},
   "source": [
    "#### Birch - didn't learned in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "Birch(n_clusters=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ddeb56",
   "metadata": {},
   "source": [
    "#### Dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcc658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_best_dbscan_params(clean_X: np.ndarray, min_samples: int):\n",
    "    range_eps = np.linspace(0.1, 10, 20)\n",
    "    scores = []\n",
    "    for eps in range_eps:\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        good_labels = model.fit_predict(clean_X)\n",
    "        noisy_data_count = len(good_labels[good_labels == -1])\n",
    "        if noisy_data_count > good_labels.shape[0] * 0.5 or len(np.unique(good_labels)) < 3:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        print(f\"noisy data count with eps={eps}: {noisy_data_count}\")\n",
    "        try:\n",
    "            score = metrics.silhouette_score(clean_X, good_labels)\n",
    "        except:\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "    plt.plot(range_eps, scores)\n",
    "    plt.show()\n",
    "    return range_eps[np.argmax(scores)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacac3f1",
   "metadata": {},
   "source": [
    "#### SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = SpectralClustering(n_clusters=2,assign_labels='discretize',random_state=0).fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9f26c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d1db6ca",
   "metadata": {},
   "source": [
    "### Associate the clusters with the ground truth, and explain what external variable is best associated with clusters, and what clustering method best associates with the outside variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb9978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c680268",
   "metadata": {},
   "source": [
    "### Find anomalies in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e5e45",
   "metadata": {},
   "source": [
    "### using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf390fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "labels = model.fit_predict(X)\n",
    "labels[labels == -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080421d",
   "metadata": {},
   "source": [
    "### using isolation forest\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8166a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = IsolationForest(random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c85c7",
   "metadata": {},
   "source": [
    "### one classs svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37141510",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class_svm = OneClassSVM(nu=0.01, kernel = 'rbf', gamma = 'auto').fit(X_train)\n",
    "prediction = one_class_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5baa46",
   "metadata": {},
   "source": [
    "### test whether anomalies are associated with any of the external variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5992ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7e6193c",
   "metadata": {},
   "source": [
    "### check if anomaly detection improves the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc8955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b0c30ee",
   "metadata": {},
   "source": [
    "### Reduce the dimension of the data and propose a visualization that best characterize the clusters associated with the external variables. Please find a smart presentation scheme to highlight both clusters and variables.\n",
    "\n",
    "- TSNE\n",
    "- Isomap\n",
    "- MDS\n",
    "- PCA\n",
    "- SpectralEmbedding\n",
    "- PCoA\n",
    "- LLE\n",
    "- UMAP\n",
    "- AutoEncoders\n",
    "- ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ccf593",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T13:48:58.474377Z",
     "start_time": "2023-02-03T13:48:58.460546Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_dimension(X):\n",
    "    models = [TSNE(n_components=2), Isomap(n_components=2), MDS(n_components=2), SpectralEmbedding(n_components=2)]\n",
    "    fig, axs = plt.subplots(nrows=len(models), ncols=1, figsize=(8, 8))\n",
    "    fig.tight_layout()\n",
    "    for i, model in tqdm(enumerate(models)):\n",
    "        X_embedded_tsne = model.fit_transform(X)\n",
    "        axs[i].scatter(X_embedded_tsne[:, 0], X_embedded_tsne[:, 1], s=40, cmap='viridis')\n",
    "        axs[i].set_title(f\"{model} dimensionality reduction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a8657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d7342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f219202",
   "metadata": {},
   "source": [
    "## Dynamic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic_dataset(data_path=\"data/driftdataset\"):\n",
    "    dfs = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        with open(os.path.join(data_path, filename), \"r\") as file:\n",
    "            df = pd.read_csv(\n",
    "                file,\n",
    "                sep=\"\\s+\",\n",
    "                skiprows=1,\n",
    "                usecols=[0, 7],\n",
    "                names=['TIME', 'XGSM']\n",
    "            )\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d98aba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T13:41:13.442198Z",
     "start_time": "2023-02-03T13:41:13.436848Z"
    }
   },
   "source": [
    "### Hidden Markov model for time series\n",
    "https://rubikscode.net/2021/09/06/stock-price-prediction-using-hidden-markov-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63959d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99de5e94",
   "metadata": {},
   "source": [
    "## Graph Based Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe0967",
   "metadata": {},
   "source": [
    "### Louvain’s Algorithm for Community Detection\n",
    "https://github.com/taynaud/python-louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb822f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:09:09.182601Z",
     "start_time": "2023-02-22T16:07:26.069402Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_AxesStack' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m graph, target \u001b[38;5;241m=\u001b[39m load_graph_dataset()\n\u001b[1;32m     14\u001b[0m pos \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mspring_layout(graph)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/networkx/drawing/nx_pylab.py:115\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(G, pos, ax, **kwds)\u001b[0m\n\u001b[1;32m    113\u001b[0m cf\u001b[38;5;241m.\u001b[39mset_facecolor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_axstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         ax \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39madd_axes((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: '_AxesStack' object is not callable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_graph_dataset(data_path=\"../data/deezer_ego_nets\"):\n",
    "    with open(os.path.join(data_path, \"deezer_edges.json\")) as f:\n",
    "        graph = nx.MultiGraph({\n",
    "            node: [element[1] for element in neighbors]\n",
    "            for node, neighbors in json.load(f).items()\n",
    "        })\n",
    "    target = pd.read_csv(os.path.join(data_path, \"deezer_target.csv\"))\n",
    "    print(graph.edges, target)\n",
    "    return graph, target\n",
    "\n",
    "\n",
    "n = 500\n",
    "graph, target = load_graph_dataset()\n",
    "pos = nx.spring_layout(graph)\n",
    "nx.draw(graph, pos, node_size = 75, alpha = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac8b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comms = community_louvain.best_partition(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_coms = np.unique(list(comms.values()))\n",
    "cmap = {\n",
    "    0 : 'maroon',\n",
    "    1 : 'teal',\n",
    "    2 : 'black', \n",
    "    3 : 'orange',\n",
    "    4 : 'green',\n",
    "    5 : 'yellow'\n",
    "}\n",
    "\n",
    "node_cmap = [cmap[v] for _,v in comms.items()]\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, node_size = 75, alpha = 0.8, node_color=node_cmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691628ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
