{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ba97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib~=3.6.3\n",
    "!pip install networkx~=2.5\n",
    "!pip install numpy~=1.22.4\n",
    "!pip install pandas~=1.3.5\n",
    "!pip install scipy~=1.9.3\n",
    "!pip install tqdm~=4.64.1\n",
    "!pip install umap-learn~=0.5.3\n",
    "!pip install python-louvain~=0.16\n",
    "!pip install fuzzy-c-means\n",
    "!pip install hmmlearn~=0.2.8\n",
    "!pip install scikit-learn~=1.1.3\n",
    "!pip install umap-learn\n",
    "!pip install seaborn~=0.12.2\n",
    "!pip install karateclub~=1.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ee1348",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, Birch, BisectingKMeans\n",
    "from sklearn.manifold import SpectralEmbedding, Isomap, MDS\n",
    "from fcmeans import FCM\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "def k_mean(k, data: np.ndarray):\n",
    "    model = KMeans(k, max_iter=1000)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def fuzzy_c_means(k, data: np.ndarray):\n",
    "    model = FCM(n_clusters=k, n_jobs=8)\n",
    "    model.fit(data)\n",
    "    return model.predict(data)\n",
    "\n",
    "\n",
    "def gaussian_mixture(k: int, data: np.ndarray):\n",
    "    model = GaussianMixture(k, max_iter=3000)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def hierarchical_clustering(k: int, data: np.ndarray):\n",
    "    model = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def birch(k: int, data: np.ndarray):\n",
    "    model = Birch(n_clusters=k)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def spectral_clustering(k: int, data: np.ndarray):\n",
    "    model = SpectralClustering(n_clusters=k, assign_labels='discretize', n_jobs=-1)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def dbscan(k: int, data: np.ndarray):\n",
    "    neigh = NearestNeighbors(n_neighbors=k)\n",
    "    nbrs = neigh.fit(data)\n",
    "    distances, _ = nbrs.kneighbors(data)\n",
    "    distances = np.mean(distances[:, 1:], axis=1)\n",
    "    cutoff = np.quantile(distances, q=0.98)\n",
    "    model = DBSCAN(eps=cutoff, min_samples=1, n_jobs=-1)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def bisecting_kmeans(k: int, data: np.ndarray):\n",
    "    model = BisectingKMeans(n_clusters=k)\n",
    "    labels = model.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "\n",
    "clustering_algorithms = {\n",
    "    \"k_mean\": k_mean,\n",
    "    \"fuzzy_c_means\": fuzzy_c_means,\n",
    "    \"gaussian_mixture\": gaussian_mixture,\n",
    "    \"hierarchical_clustering\": hierarchical_clustering,\n",
    "    \"birch\": birch,\n",
    "    \"dbscan\": dbscan,\n",
    "    \"bisecting_kmeans\": bisecting_kmeans\n",
    "}\n",
    "\n",
    "dim_reduction_algorithms = {\n",
    "    \"MDS\": lambda k: MDS(n_components=k, n_jobs=-1, max_iter=300),\n",
    "    # \"PCA\": lambda k: PCA(n_components=k),\n",
    "    # \"FastICA\": lambda k: FastICA(n_components=k, max_iter=200),\n",
    "    \"without_reduction\": None,\n",
    "    # \"Isomap\": lambda k: Isomap(n_components=k, n_jobs=-1, max_iter=200),\n",
    "    # \"SpectralEmbedding\": lambda k: SpectralEmbedding(n_components=k, n_jobs=-1),\n",
    "    # \"LLE\": lambda k: LocallyLinearEmbedding(n_components=k, n_jobs=-1),\n",
    "    # \"UMAP\": lambda k: UMAP(n_neighbors=100, n_components=k, n_epochs=1000, init='spectral', low_memory=False, verbose=False)\n",
    "}\n",
    "\n",
    "anomaly_detection_algorithms = {\n",
    "    \"without_anomaly\": None,\n",
    "    \"OneClassSVM\": OneClassSVM(kernel=\"rbf\", nu=0.01, gamma='scale'),\n",
    "    \"IsolationForest\": IsolationForest(random_state=0, n_jobs=-1, n_estimators=500, max_samples=256),\n",
    "    \"DBSCAN\": DBSCAN(eps=5, min_samples=5, n_jobs=-1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_algo(scores_mapping: Dict[Any, List[float]]) -> Tuple[Any, float, float, str]:\n",
    "    _, p_value = f_oneway(*list(scores_mapping.values()))\n",
    "    best_algo = random.choice(list(scores_mapping.keys()))\n",
    "    t_test_p_value = -1\n",
    "    print(f\"annova value: {p_value}\")\n",
    "    msg = \"\"\n",
    "    if p_value < P_VALUE_THR:\n",
    "        sorted_scores = sorted(\n",
    "            scores_mapping,\n",
    "            key=lambda key: np.mean(scores_mapping[key]),\n",
    "            reverse=True\n",
    "        )\n",
    "        candidate1, candidate2 = sorted_scores[:2]\n",
    "        _, t_test_p_value = ttest_rel(\n",
    "            scores_mapping[candidate1],\n",
    "            scores_mapping[candidate2]\n",
    "        )\n",
    "        print(f\"t_test value: {t_test_p_value}\")\n",
    "        best_algo = sorted_scores[0]\n",
    "        if t_test_p_value >= P_VALUE_THR:\n",
    "            msg = f\"followed by t-test: algorithms {candidate1}, {candidate2} are the same\"\n",
    "            print(msg)\n",
    "    else:\n",
    "        msg = f\"followed by annova: algorithms {scores_mapping.keys()} are the same\"\n",
    "        print(msg)\n",
    "    return best_algo, p_value, t_test_p_value, msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fad45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Any, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, mutual_info_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "P_VALUE_THR = 0.05\n",
    "\n",
    "\n",
    "def get_silhouette_scores(\n",
    "        X_cvs: List[np.ndarray],\n",
    "        clustering_algo_name: str, reduction_algo_name: str,\n",
    "        dim_num: int, k_clusters: int\n",
    ") -> List[float]:\n",
    "    scores = []\n",
    "    for cv_id, cv_data in enumerate(X_cvs):\n",
    "        try:\n",
    "            cv_data = reduction_algo_wrapper(reduction_algo_name, dim_num, cv_data, cv_id)\n",
    "            print(f\"doing {clustering_algo_name} for {cv_data.shape} by {reduction_algo_name}\")\n",
    "            labels = clustering_algorithms[clustering_algo_name](k_clusters, cv_data)\n",
    "            # print(f\"anomaly: {labels[labels == -1].size / labels.size}\")\n",
    "            scores.append(silhouette_score(cv_data[labels != -1], labels[labels != -1]))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "\n",
    "def reduction_algo_wrapper(reduction_algo_name: str, dim_num: int, cv_data: np.ndarray, cv_id: int, CACHE_PATH) -> np.ndarray:\n",
    "    reduction_algo = dim_reduction_algorithms[reduction_algo_name]\n",
    "    if reduction_algo is None:\n",
    "        return cv_data\n",
    "    cache_file = os.path.join(CACHE_PATH, f\"{reduction_algo_name}-{dim_num}-{cv_id}.pkl\")\n",
    "    if Path(cache_file).is_file():\n",
    "        with open(cache_file, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "    cv_data = reduction_algo(dim_num).fit_transform(cv_data)\n",
    "    with open(cache_file, \"wb\") as file:\n",
    "        pickle.dump(cv_data, file)\n",
    "    return cv_data\n",
    "\n",
    "\n",
    "def generate_cvs(X: np.ndarray, y: pd.DataFrame, num_of_cvs, cv_size) -> Tuple[List[np.ndarray], List[pd.DataFrame]]:\n",
    "    X_cvs = []\n",
    "    y_cvs = []\n",
    "    for i in range(num_of_cvs):\n",
    "        rows = np.random.randint(X.shape[0], size=cv_size)\n",
    "        X_cvs.append(X[rows, :])\n",
    "        y_cvs.append(y.iloc[rows])\n",
    "    return X_cvs, y_cvs\n",
    "\n",
    "\n",
    "def find_best_algo(scores_mapping: Dict[Any, List[float]]) -> Tuple[Any, float, float, str]:\n",
    "    _, p_value = f_oneway(*list(scores_mapping.values()))\n",
    "    best_algo = random.choice(list(scores_mapping.keys()))\n",
    "    t_test_p_value = -1\n",
    "    print(f\"annova value: {p_value}\")\n",
    "    msg = \"\"\n",
    "    if p_value < P_VALUE_THR:\n",
    "        sorted_scores = sorted(\n",
    "            scores_mapping,\n",
    "            key=lambda key: np.mean(scores_mapping[key]),\n",
    "            reverse=True\n",
    "        )\n",
    "        candidate1, candidate2 = sorted_scores[:2]\n",
    "        _, t_test_p_value = ttest_rel(\n",
    "            scores_mapping[candidate1],\n",
    "            scores_mapping[candidate2]\n",
    "        )\n",
    "        print(f\"t_test value: {t_test_p_value}\")\n",
    "        best_algo = sorted_scores[0]\n",
    "        if t_test_p_value >= P_VALUE_THR:\n",
    "            msg = f\"followed by t-test: algorithms {candidate1}, {candidate2} are the same\"\n",
    "            print(msg)\n",
    "    else:\n",
    "        msg = f\"followed by annova: algorithms {scores_mapping.keys()} are the same\"\n",
    "        print(msg)\n",
    "    return best_algo, p_value, t_test_p_value, msg\n",
    "\n",
    "\n",
    "def external_var_to_anomalies(X_cvs, y_cvs, external_vars):\n",
    "    results = list()\n",
    "    for anomaly_algo_name, anomaly_algo in tqdm(anomaly_detection_algorithms.items()):\n",
    "        if anomaly_algo is None:\n",
    "            continue\n",
    "        scores = defaultdict(list)\n",
    "        for X, y in zip(X_cvs, y_cvs):\n",
    "            labels = anomaly_algo.fit_predict(X)\n",
    "            for external_var_name in external_vars:\n",
    "                scores[external_var_name].append(\n",
    "                    mutual_info_score(\n",
    "                        labels,\n",
    "                        y[external_var_name]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for external_var_name, external_var_scores in scores.items():\n",
    "            results.append({\n",
    "                \"algo_name\": anomaly_algo_name,\n",
    "                \"external_var\": external_var_name,\n",
    "                \"MI\": np.mean(external_var_scores)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379bf6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "\n",
    "def hierarchical_clustering_vis(X, ):\n",
    "    sch.dendrogram(sch.linkage(X[np.random.randint(X.shape[0], size=100), :], method='ward'))\n",
    "    plt.title(\"hierarchical clustering - dendrogram\")\n",
    "    plt.plot()\n",
    "\n",
    "\n",
    "def elbow_method(X: np.ndarray, labels: np.ndarray, n_clusters: int):\n",
    "    sample_silhouette_values = silhouette_samples(X, labels)\n",
    "    y_lower = 10\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        plt.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        # TODO: legend instead of text\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    plt.yticks([])\n",
    "    plt.savefig(\"elbow.png\")\n",
    "\n",
    "\n",
    "# def reduce_dimension(X):\n",
    "#     models = [TSNE(n_components=2), Isomap(n_components=2), MDS(n_components=2), SpectralEmbedding(n_components=2)]\n",
    "#     fig, axs = plt.subplots(nrows=len(models), ncols=1, figsize=(8, 8))\n",
    "#     fig.tight_layout()\n",
    "#     for i, model in tqdm(enumerate(models)):\n",
    "#         X_embedded_tsne = model.fit_transform(X)\n",
    "#         axs[i].scatter(X_embedded_tsne[:, 0], X_embedded_tsne[:, 1], s=40, cmap='viridis')\n",
    "#         axs[i].set_title(f\"{model} dimensionality reduction\")\n",
    "#     plt.show()\n",
    "\n",
    "def anomaly_external_var_to_mi(df):\n",
    "    g = sns.catplot(\n",
    "        data=df, kind=\"bar\",\n",
    "        x=\"external_var\", y=\"MI\", hue=\"algo_name\",\n",
    "        errorbar=\"sd\", palette=\"dark\", alpha=.6, height=6\n",
    "    )\n",
    "    g.despine(left=True)\n",
    "    g.set_axis_labels(\"External Variable\", \"Mutual Information\")\n",
    "    g.legend.set_title(\"Anomaly MI Per External Var\")\n",
    "    plt.savefig(\"anomaly_external_var_to_mi.png\")\n",
    "\n",
    "\n",
    "def plot_silhouette():\n",
    "    with open(\"./reports/without_anomaly_algo/silout_per_clustreing.json\", \"r\") as file:\n",
    "        without_anomaly_algo_scores = json.load(file)\n",
    "    with open(\"./reports/with_anomaly_algo/sillout_pre_clustering.json\", \"r\") as file:\n",
    "        with_anomaly_algo_scores = json.load(file)\n",
    "    rows = []\n",
    "    for key, scores in without_anomaly_algo_scores.items():\n",
    "        for score in scores:\n",
    "            rows.append({\n",
    "                \"Clustering Algorithm\": key.replace(\"_\", \" \").title(),\n",
    "                \"score\": score,\n",
    "                \"With Anomaly Filtering\": \"No\"\n",
    "            })\n",
    "    for key, scores in with_anomaly_algo_scores.items():\n",
    "        for score in scores:\n",
    "            rows.append({\n",
    "                \"Clustering Algorithm\": key.replace(\"_\", \" \").title(),\n",
    "                \"score\": score,\n",
    "                \"With Anomaly Filtering\": \"Yes\"\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    g = sns.barplot(\n",
    "        data=df, x=\"Clustering Algorithm\", y=\"score\",\n",
    "        hue=\"With Anomaly Filtering\", errorbar=\"sd\",  palette=\"dark\", alpha=.6,\n",
    "    )\n",
    "    g.set_ylabel(\"Silhouette Scores\")\n",
    "    g.set_xlabel(\"\")\n",
    "    g.set_title(\"Silhouette Scores By Algorithm\")\n",
    "    plt.savefig(\"static_silhouettes.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50155a4f",
   "metadata": {},
   "source": [
    "## dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db82518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "from sklearn.metrics import normalized_mutual_info_score, mutual_info_score, silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_PATH = \"cache-dynamic-new/\"\n",
    "Path(CACHE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "EXTERNAL_VARS = [\"gas_type\", \"concentration\"]\n",
    "DIMENTIONS_OPTIONS = [2, 10]\n",
    "NUM_CLUSTERS_OPTIONS = [2, 6, 12, 20]\n",
    "NUM_OF_CVS = 3\n",
    "# CV_SIZE = 2600\n",
    "DATA_PATH = \"data/driftdataset\"\n",
    "\n",
    "random.seed(10)\n",
    "cvs = [[\n",
    "    random.randint(0, 9),\n",
    "    random.randint(0, 9),\n",
    "    random.randint(0, 9),\n",
    "    random.randint(0, 9)\n",
    "]\n",
    "    for i in range(NUM_OF_CVS)\n",
    "]\n",
    "\n",
    "\n",
    "def load_dynamic_dataset():\n",
    "    X_cvs, y_cvs = [], []\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        with open(os.path.join(DATA_PATH, filename), \"r\") as file:\n",
    "            df_rows = []\n",
    "            for line in file.readlines():\n",
    "                curr_row = {}\n",
    "                line = line.split(\";\")\n",
    "                curr_row[\"gas_type\"] = line[0]\n",
    "                line = line[1].split(\" \")\n",
    "                curr_row[\"concentration\"] = line[0]\n",
    "\n",
    "                for sensor_value in line[1:]:\n",
    "                    sensor_value = sensor_value.split(\":\")\n",
    "                    if len(sensor_value) == 2:\n",
    "                        curr_row[f\"sensor_{sensor_value[0]}\"] = float(sensor_value[1])\n",
    "                df_rows.append(curr_row)\n",
    "            df = pd.DataFrame(df_rows)\n",
    "            X = df.drop(EXTERNAL_VARS, axis=1).values\n",
    "            y = df[EXTERNAL_VARS]\n",
    "            y[\"concentration\"] = y[\"concentration\"].apply(lambda val: int(float(val)))\n",
    "            X_cvs.append(X)\n",
    "            y_cvs.append(y)\n",
    "    return X_cvs, y_cvs\n",
    "\n",
    "\n",
    "def update_scores(labels: np.ndarray, cv_data, cv_y: pd.DataFrame, lengths, scores: Dict[str, List[float]]):\n",
    "    sil_score = silhouette_score(cv_data, labels)\n",
    "    model = CategoricalHMM(n_components=cv_y[\"gas_type\"].nunique()).fit(labels.reshape(-1, 1), lengths=lengths)\n",
    "    hidden_states_gas_type = model.predict(labels.reshape(-1, 1))\n",
    "    mi_gas_type_mi = normalized_mutual_info_score(\n",
    "        cv_y[\"gas_type\"].values,\n",
    "        hidden_states_gas_type\n",
    "    )\n",
    "    model = CategoricalHMM(n_components=cv_y[\"concentration\"].nunique()).fit(labels.reshape(-1, 1), lengths=lengths)\n",
    "    hidden_states_concentration = model.predict(labels.reshape(-1, 1))\n",
    "    mi_concentration = normalized_mutual_info_score(\n",
    "        cv_y[\"concentration\"].values,\n",
    "        hidden_states_concentration\n",
    "    )\n",
    "    scores[\"mi_concentration_scores\"].append(mi_concentration)\n",
    "    scores[\"mi_gas_type_scores\"].append(mi_gas_type_mi)\n",
    "    scores[\"weighted_scores\"].append(\n",
    "        (mi_concentration + mi_gas_type_mi + (sil_score + 1) / 2) / 2\n",
    "    )\n",
    "    return scores\n",
    "\n",
    "\n",
    "def main():\n",
    "    X_cvs, y_cvs = load_dynamic_dataset()\n",
    "    best_config_by_clustering = dict()\n",
    "    for clustering_algo_name in clustering_algorithms.keys():\n",
    "        dim_reduction_meta: Dict[str, Dict[str, Any]] = dict()\n",
    "        for reduction_algo_name in dim_reduction_algorithms.keys():\n",
    "            max_score = float(\"-inf\")\n",
    "            for dim_num in DIMENTIONS_OPTIONS:\n",
    "                for k_clusters in NUM_CLUSTERS_OPTIONS:\n",
    "                    scores = defaultdict(list)\n",
    "                    for cv_id, in_index in enumerate(cvs):\n",
    "                        cv_data = np.concatenate([X_cvs[i] for i in range(len(X_cvs)) if i in in_index])\n",
    "                        cv_y = pd.concat([y_cvs[i] for i in range(len(y_cvs)) if i in in_index])\n",
    "                        lengths = [X_cvs[i].shape[0] for i in range(len(X_cvs)) if i in in_index]\n",
    "                        try:\n",
    "                            cv_data = reduction_algo_wrapper(reduction_algo_name, dim_num, cv_data, cv_id, CACHE_PATH)\n",
    "                            print(f\"doing {clustering_algo_name} for {cv_data.shape} by {reduction_algo_name}\")\n",
    "                            labels = clustering_algorithms[clustering_algo_name](k_clusters, cv_data)\n",
    "                            scores = update_scores(labels, cv_data, cv_y, lengths, scores)\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            break\n",
    "                    avg_score = np.mean(scores[\"weighted_scores\"])\n",
    "                    if avg_score > max_score:\n",
    "                        max_score = avg_score\n",
    "                        dim_reduction_meta[reduction_algo_name] = {\n",
    "                            **scores,\n",
    "                            \"avg_score\": avg_score,\n",
    "                            \"dim_num\": dim_num,\n",
    "                            \"cluster_num\": k_clusters,\n",
    "                            \"reduction_algo_name\": reduction_algo_name\n",
    "                        }\n",
    "        best_algo_name, p_value, t_test_p_value, msg = find_best_algo({\n",
    "            key: value[\"weighted_scores\"] for key, value in dim_reduction_meta.items()\n",
    "        })\n",
    "        best_config_by_clustering[clustering_algo_name] = {\n",
    "            **dim_reduction_meta[best_algo_name],\n",
    "            \"annova\": p_value,\n",
    "            \"t-test\": t_test_p_value,\n",
    "            \"msg\": msg\n",
    "        }\n",
    "        print(f\"picking for {clustering_algo_name}: {best_config_by_clustering[clustering_algo_name]}\")\n",
    "    print(best_config_by_clustering)\n",
    "    best_config_by(\"weighted_scores\", best_config_by_clustering)\n",
    "    best_config_by(\"mi_gas_type_scores\", best_config_by_clustering)\n",
    "    best_config_by(\"mi_concentration_scores\", best_config_by_clustering)\n",
    "    find_best_external_var_per_clustering(X_cvs, y_cvs, best_config_by_clustering)\n",
    "\n",
    "\n",
    "def find_best_external_var_per_clustering(X_cvs: List[np.ndarray], y_cvs: List[pd.DataFrame],\n",
    "                                          best_config_by_clustering: Dict[str, Dict[str, Any]]):\n",
    "    best_external_var_per_clustering = dict()\n",
    "    for clustering_algo_name, best_config in best_config_by_clustering.items():\n",
    "        all_mi = dict()\n",
    "        print(f\"-----------{clustering_algo_name}-------\")\n",
    "        for external_var_name in EXTERNAL_VARS:\n",
    "            scores = []\n",
    "            clustering_algo = clustering_algorithms[clustering_algo_name]\n",
    "            for cv_id, in_index in enumerate(cvs):\n",
    "                cv_data = np.concatenate([X_cvs[i] for i in range(len(X_cvs)) if i in in_index])\n",
    "                cv_y = pd.concat([y_cvs[i] for i in range(len(y_cvs)) if i in in_index])\n",
    "                lengths = [X_cvs[i].shape[0] for i in range(len(X_cvs)) if i in in_index]\n",
    "                try:\n",
    "                    cv_data = reduction_algo_wrapper(\n",
    "                        best_config[\"reduction_algo_name\"],\n",
    "                        best_config[\"dim_num\"],\n",
    "                        cv_data, cv_id, CACHE_PATH\n",
    "                    )\n",
    "                    labels = clustering_algo(cv_y[external_var_name].nunique(), cv_data)\n",
    "                    model = CategoricalHMM(\n",
    "                        n_components=cv_y[external_var_name].nunique()\n",
    "                    ).fit(\n",
    "                        labels.reshape(-1, 1),\n",
    "                        lengths=lengths\n",
    "                    )\n",
    "                    hidden_states = model.predict(labels.reshape(-1, 1))\n",
    "                    scores.append(mutual_info_score(hidden_states, cv_y[external_var_name].values))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    scores.append(-1)\n",
    "            all_mi[external_var_name] = scores\n",
    "        best_var, p_value, t_test_p_value, msg = find_best_algo(all_mi)\n",
    "        best_external_var_per_clustering[clustering_algo_name] = {\n",
    "            \"scores\": all_mi[best_var],\n",
    "            \"best_var\": best_var,\n",
    "            \"anova\": p_value,\n",
    "            \"t_test\": t_test_p_value,\n",
    "            \"msg\": msg\n",
    "        }\n",
    "    print(best_external_var_per_clustering)\n",
    "    with open(f\"best_external_var_per_clustering_dynamic.json\", \"w\") as file:\n",
    "        json.dump(best_external_var_per_clustering, file)\n",
    "    return best_external_var_per_clustering\n",
    "\n",
    "\n",
    "def best_config_by(key: str, best_config_by_clustering: Dict):\n",
    "    clustering_scores = dict()\n",
    "    for clustering_algo_name, metadata in best_config_by_clustering.items():\n",
    "        clustering_scores[clustering_algo_name] = metadata[key]\n",
    "    best_algo_name, p_value, t_test_p_value, msg = find_best_algo(clustering_scores)\n",
    "    result = {\n",
    "        \"best_algo_name\": best_algo_name,\n",
    "        \"annova\": p_value,\n",
    "        \"t_test_p_value\": t_test_p_value,\n",
    "        \"config\": best_config_by_clustering[best_algo_name],\n",
    "        \"msg\": msg,\n",
    "        \"all_config\": best_config_by_clustering\n",
    "    }\n",
    "    print(f\"---------final_result_{key}--------\")\n",
    "    print(result)\n",
    "    with open(f\"final_result_{key}.json\", \"w\") as file:\n",
    "        json.dump(result, file)\n",
    "\n",
    "\n",
    "def external_var_to_anomalies(X_cvs, y_cvs, external_vars):\n",
    "    results = list()\n",
    "    for anomaly_algo_name, anomaly_algo in tqdm(anomaly_detection_algorithms.items()):\n",
    "        if anomaly_algo is None:\n",
    "            continue\n",
    "        scores = defaultdict(list)\n",
    "        for cv_id, in_index in enumerate(cvs):\n",
    "            cv_data = np.concatenate([X_cvs[i] for i in range(len(X_cvs)) if i in in_index])\n",
    "            cv_y = pd.concat([y_cvs[i] for i in range(len(y_cvs)) if i in in_index])\n",
    "            lengths = [X_cvs[i].shape[0] for i in range(len(X_cvs)) if i in in_index]\n",
    "            labels = anomaly_algo.fit_predict(cv_data)\n",
    "            for external_var_name in external_vars:\n",
    "                model = CategoricalHMM(n_components=cv_y[external_var_name].nunique()).fit(\n",
    "                    labels.reshape(-1, 1),\n",
    "                    lengths=lengths\n",
    "                )\n",
    "                labels = model.predict(labels.reshape(-1, 1))\n",
    "                scores[external_var_name].append(\n",
    "                    mutual_info_score(\n",
    "                        labels,\n",
    "                        cv_y[external_var_name]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for external_var_name, external_var_scores in scores.items():\n",
    "            results.append({\n",
    "                \"algo_name\": anomaly_algo_name,\n",
    "                \"external_var\": external_var_name,\n",
    "                \"MI\": np.mean(external_var_scores)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def main2():\n",
    "    X_cvs, y_cvs = load_dynamic_dataset()\n",
    "    df = external_var_to_anomalies(X_cvs, y_cvs, EXTERNAL_VARS)\n",
    "    anomaly_external_var_to_mi(df)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "\n",
    "main2()\n",
    "# main3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f66b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-04T23:21:45.827976Z",
     "start_time": "2023-03-04T23:21:45.818923Z"
    }
   },
   "source": [
    "## graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80239528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from community import community_louvain\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, normalized_mutual_info_score, mutual_info_score\n",
    "from tqdm import tqdm\n",
    "from karateclub.graph_embedding import Graph2Vec\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sknetwork.embedding import LouvainEmbedding, LouvainNE\n",
    "\n",
    "DATA_PATH = \"data/deezer_ego_nets\"\n",
    "DIMENTIONS_OPTIONS = [10, 50, 100]\n",
    "NUM_CLUSTERS_OPTIONS = [2, 4, 8, 12, 16, 20]\n",
    "NUM_OF_CVS = 5\n",
    "CV_SIZE = 2000\n",
    "p_value_thr = 0.05\n",
    "\n",
    "\n",
    "def preprocess_data():\n",
    "    graphs = []\n",
    "    with open(os.path.join(DATA_PATH, \"deezer_edges.json\")) as f:\n",
    "        graphs_dict: Dict = json.load(f)\n",
    "    for graph_id, edges in graphs_dict.items():\n",
    "        curr_graph = nx.Graph()\n",
    "        for u, v in edges:\n",
    "            curr_graph.add_edge(u, v)\n",
    "        graphs.append(curr_graph)\n",
    "    return graphs\n",
    "    # adj_matrix = np.zeros((n, n))\n",
    "    # for i in range(n):\n",
    "    #     for _, edge in graph_dict[str(i)]:\n",
    "    #         adj_matrix[i, edge] = 1\n",
    "    # data = PCA(n_components=0.98).fit_transform(adj_matrix)\n",
    "    # with open(\"data.pkl\", \"wb\") as file:\n",
    "    #     pickle.dump(data, file)\n",
    "    # return data\n",
    "\n",
    "\n",
    "def main_flow():\n",
    "    with open(\"data.pkl\", \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    print(data.shape)\n",
    "    target = pd.read_csv(os.path.join(DATA_PATH, \"deezer_target.csv\"))[\"target\"]\n",
    "    X_cvs, y_cvs = generate_cvs(data, target, NUM_OF_CVS, CV_SIZE)\n",
    "    best_config_by_clustering = dict()\n",
    "    for clustering_algo_name in clustering_algorithms.keys():\n",
    "        scores_by_k = dict()\n",
    "        for k_clusters in NUM_CLUSTERS_OPTIONS:\n",
    "            silhouette_scores = []\n",
    "            mi_scores = []\n",
    "            scores = []\n",
    "            for cv_id, (cv_data, cv_y) in enumerate(zip(X_cvs, y_cvs)):\n",
    "                try:\n",
    "                    labels = clustering_algorithms[clustering_algo_name](k_clusters, cv_data)\n",
    "                    sil_score = silhouette_score(cv_data[labels != -1], labels[labels != -1])\n",
    "                    silhouette_scores.append(sil_score)\n",
    "                    sil_score = (sil_score + 1) / 2  # normalize between 0 and 1\n",
    "                    mi_score = normalized_mutual_info_score(labels, cv_y.values)\n",
    "                    scores.append((sil_score + mi_score) / 2)\n",
    "                    mi_scores.append(mutual_info_score(labels, cv_y.values))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    break\n",
    "            scores_by_k[k_clusters] = {\n",
    "                \"scores\": scores,\n",
    "                \"mi_scores\": mi_scores,\n",
    "                \"silhouette_scores\": silhouette_scores\n",
    "            }\n",
    "        best_k_clusters, p_value, t_test_p_value, msg = find_best_algo({\n",
    "            key: value[\"scores\"] for key, value in scores_by_k.items()\n",
    "        })\n",
    "        best_config_by_clustering[clustering_algo_name] = {\n",
    "            **scores_by_k[best_k_clusters],\n",
    "            \"best_k_clusters\": best_k_clusters,\n",
    "            \"annova\": p_value,\n",
    "            \"t_test_p_value\": t_test_p_value\n",
    "        }\n",
    "        print(f\"picking for {clustering_algo_name}: {best_k_clusters}\")\n",
    "    print(best_config_by_clustering)\n",
    "    # with open(\"best_config_by_clustering.json\") as file:\n",
    "    #     json.dump(best_config_by_clustering, file)\n",
    "    clustering_scores = dict()\n",
    "    for clustering_algo_name, metadata in best_config_by_clustering.items():\n",
    "        clustering_scores[clustering_algo_name] = metadata[\"scores\"]\n",
    "    best_k_clusters, p_value, t_test_p_value, msg = find_best_algo(clustering_scores)\n",
    "    print(f\"best_k_clusters: {best_k_clusters}\")\n",
    "    print(f\"annova: {p_value}\")\n",
    "    print(f\"t test: {t_test_p_value}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    graphs = preprocess_data()\n",
    "    target = pd.read_csv(os.path.join(DATA_PATH, \"deezer_target.csv\"))[\"target\"].values\n",
    "    # louvain = LouvainEmbedding()\n",
    "    louvain = LouvainNE(n_components=10)\n",
    "    results = []\n",
    "    max_size = 0\n",
    "    for graph in tqdm(graphs):\n",
    "        embedding = louvain.fit_transform(nx.adjacency_matrix(graph))\n",
    "        # embedding = louvain.fit_transform(nx.adjacency_matrix(graph))\n",
    "        embedding = embedding.mean(axis=0)\n",
    "        results.append(embedding)\n",
    "        max_size = max(max_size, embedding.shape[0])\n",
    "    # for i in range(len(results)):\n",
    "    #     num_zeros = max_size - results[i].shape[0]\n",
    "    #     if num_zeros > 0:\n",
    "    #         results[i] = np.pad(results[i], (0, num_zeros), 'constant')\n",
    "    data = np.vstack(results)\n",
    "    print(\"doing k means\")\n",
    "    scores = []\n",
    "    for k in tqdm([2, 6, 12, 20]):\n",
    "        labels = hierarchical_clustering(k, data)\n",
    "        scores.append({\n",
    "            \"silhouette_score\": silhouette_score(data, labels),\n",
    "            \"mutual_info_score\": normalized_mutual_info_score(labels, target)\n",
    "        })\n",
    "    print(scores)\n",
    "\n",
    "\n",
    "main()\n",
    "# main_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
